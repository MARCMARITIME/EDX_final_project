---
title: "Wind Turbine Project"
author: "Marc-Eloi NICOD"
date: "`r Sys.Date()`"
output:
  html_document: default
  pdf_document: default
editor_options: 
  markdown: 
    wrap: 72
---

# Wind Turbine project

## 1) Data description

The project is based on the iot data made publicly available by Engie
for 1 wind turbine in France in La Haute Borne with data from January
2013 to January 2018 collected with a frequency of 10 minutes. Link:
<https://opendata-renewables.engie.com/explore/index> (warning: Engie's
'open data page' is not always up-and-running, but all the necessary
info is also included in the shared documents.)

The data set has been enriched with certain additional parameters such
as snowfall_1h, rainfall_1h, air density, humidity etc. This
'enrichment' was performed using historic data bought from
<https://openweathermap.org/>

## 2) Introduction

The aim of this project is to predict the production of Power in Kw of
one of the wind turbine named R80711 using weather parameters.

Two models will be developed.

The developed models should predict the power generated of wind turbines
based on different weather parameters. The metric used to evaluate the
performance is the RMSE (root mean squared error), which should be
minimized. The models will be based on the wind turbines data set that
contains 261812 observations, which will be split into two parts: The
training set, used for training and evaluation of the model,and the test
set, which will be used to evaluate the final model.

------------------------------------------------------------------------

## 3) Methods

The first part of the report consists of an exploratory data analysis of
the data set. This analysis is required to identify variables which can
be used to optimize the model.

The second part of the report consists of the creation of the prediction
system.

To ensure the integrity of our training and testing datasets and to
mitigate the potential for data leakage due to the close temporal
proximity of observations (10 minutes between observations), we will
employ a time-based split rather than a random shuffle. The wind turbine
dataset will be arranged chronologically based on timestamps. The first
80% of the chronologically ordered data will be designated as the
training dataset, while the remaining 20% will be set aside as the test
dataset. This ensures that the training data and testing data come from
distinct periods, reducing the risk of unintentionally training the
model on information that would be found in the test set.

For the first model, the dataset will be processed using the Min-Max
Normalization to normalize the features. Subsequently, the
XGBoostRegressor will be employed to predict the power generated by the
wind turbine based on the weather parameters identified during
exploratory data analysis. In contrast, the second model will again
utilize the Min-Max Normalization for normalization but will leverage
the LightGBM for predictions. In the third phase of the project, the
efficacy of both models will be tested on the validation set to evaluate
their performance in estimating wind turbine power output.\
We will also use cross validation to tune hyper parameter of
XGBoostRegressor.

## 4) Executive summary:

In a thorough analysis, we ascertained that weather parameters
effectively forecast power output with the LightGBM and XGBoost models,
displaying normalized RMSEs of 69% and 71% respectively. Yet, while
observed weather data was consistent, transitioning to weather forecasts
may bring in uncertainties that can impact predictions. Even with these
nuances, this research highlights the potential of machine learning in
advancing renewable energy forecasting.

## 5) Exploratory Data Analysis and cleaning

```{r message=FALSE, warning=FALSE, include=FALSE}
library(tidyverse)
library(knitr)

# Get the current working directory
current_dir <- getwd()

# Construct the path to the zip file
zip_file_path <- file.path(current_dir, "R80711.zip")

# Unzip the file in the current directory
unzip(zip_file_path, exdir = current_dir)

# Construct the path to the CSV file inside the zip
csv_file_path <- file.path(current_dir, "R80711.csv")

# Read the CSV file
wind_turbine <- read.csv(csv_file_path)

# View the first few rows of the data
head(wind_turbine)

```

We start with a exploratory analysis of the wind_turbine data set to
identify variables which can be used to optimize the model.

```{r, echo = FALSE}
head(wind_turbine) %>% knitr::kable()
```

The dataset consists of `r ncol(wind_turbine)` variables with
`r nrow(wind_turbine)` observations.

The dataset contains the following variables:"Date_time",
"Date_time_nr", "Wind_turbine_name", "Ba_avg", "P_avg", "Q_avg",
"Ya_avg", "Yt_avg", "Ws1_avg", "Ws2_avg", "Ws_avg", "Wa_avg", "Va_avg",
"Ot_avg", "Rs_avg", "Rbt_avg", "Rm_avg", "temp", "pressure", "humidity",
"wind_speed", "wind_deg", "rain_1h", and "snow_1h". For our analysis, we
will focus on the weather-related variables we added, using them to
predict the power output represented by "P_avg".

Let's verify if there is some missing values or NA:

```{r echo=TRUE}

missing_values <- apply(is.na(wind_turbine), 2, sum)

cat(missing_values)

```

As we can see there is no missing values.

Let's check columns names:

```{r echo=TRUE}
cat(colnames(wind_turbine))

```

Let's see what are our weather parameters values using histograms:

```{r echo=FALSE}
library(ggplot2)

# List of variables you want to visualize
variables <- c("temp", "pressure", "humidity", "wind_speed", "wind_deg", "rain_1h", "snow_1h")

# Generate histograms for each variable
lapply(variables, function(var) {
  ggplot(wind_turbine, aes_string(x = var)) + 
    geom_histogram(fill = "skyblue", color = "black", bins = 30) + 
    labs(title = paste("Histogram of", var),
         x = var,
         y = "Frequency") + 
    theme_minimal()
})

```

From the histograms above, no significant patterns stand out. Let's now
focus on the relationship between power generated by wind turbines and
wind speed, as it's likely our primary correlation:

```{r, echo = FALSE}

wind_turbine %>% 
  ggplot(aes(x = P_avg)) + 
  geom_histogram(binwidth = 10, fill = "blue", color = "black", alpha = 0.7) +
  labs(title = "Histogram of P_avg", x = "Power (P_avg)", y = "Frequency") +
  theme_minimal()

```

From this histogram, it seems that there is some negative value for
Power output. As it's not possible, we know can remove some negative
(impossible) value and check again the same plot with our new data set
cleaned "wind_turbine_clean_1".\
I will also remove 0 values from the plot for more visibility:

```{r echo=TRUE}
library(dplyr)

wind_turbine_clean_1 <- wind_turbine %>% 
  filter(P_avg >= 0)

wind_turbine_clean_1 %>% 
  filter(P_avg > 0) %>% 
  ggplot(aes(x = P_avg)) + 
  geom_histogram(binwidth = 10, fill = "blue", color = "black", alpha = 0.7) +
  labs(title = "Histogram of P_avg", x = "Power (P_avg)", y = "Frequency") +
  theme_minimal()

```

**Wind speed effect on Power output:**

Of course wind speed will have an important effect on the power
generated. Let's plot it:

```{r, echo = FALSE}

wind_turbine %>% 
  ggplot(aes(x = wind_speed, y = P_avg)) + 
  geom_point(aes(color = wind_speed), alpha = 0.6) + 
  labs(title = "Relationship between Wind Speed and Power Output (P_avg)",
       x = "Wind Speed",
       y = "Power Output (P_avg)") +
  theme_minimal() +
  scale_color_gradient(low = "blue", high = "red")

```

From the graph above we can see clearly a pattern between wind speed and
power output.\
I propose to visualize also with a graph showing the density of points
so it may be more clean to see the pattern:

```{r echo=FALSE}
library(hexbin) 

wind_turbine_clean_1 %>% 
  ggplot(aes(x = wind_speed, y = P_avg)) + 
  geom_hex(aes(fill = after_stat(density)), bins = 30) + 
  scale_fill_viridis_c() + 
  labs(title = "Relationship between Wind Speed and Power Output (P_avg)",
       x = "Wind Speed",
       y = "Power Output (P_avg)") +
  theme_minimal()
```

Let's create similar visualizations for the other weather variables:
temperature, pressure, humidity, wind direction, rainfall in the last
hour, and snowfall in the last hour\

```{r echo=FALSE}

# Define a function to generate scatter plots for a given variable
generate_scatter_plot <- function(data, var_name) {
  var_sym <- sym(var_name)
  
  ggplot(data, aes(x = !!var_sym, y = P_avg)) + 
    geom_point(aes(color = !!var_sym), alpha = 0.6) +
    scale_color_gradient(low = "blue", high = "red") + 
    labs(title = paste("Relationship between", var_name, "and Power Output (P_avg)"),
         x = var_name,
         y = "Power Output (P_avg)") +
    theme_minimal()
}

# List of variables to iterate over
variables <- c("temp", "pressure", "humidity", "wind_deg", "rain_1h", "snow_1h")

# Generate and display plots for each variable
plot_list <- lapply(variables, function(var) generate_scatter_plot(wind_turbine_clean_1, var))

# Display each plot in the plot_list
for (p in plot_list) {
  print(p)
}
```

## 6) Modelisation

In our analysis, we chose to utilize the XGBoost and LightGBM
algorithms, primarily because both are renowned for their efficiency and
accuracy in handling regression tasks, especially with large datasets.
They are gradient boosting frameworks that employ tree-based learning
algorithms and have proven to deliver state-of-the-art results across
various domains.

The normalization of data, specifically using the range method, is a
crucial pre-processing step. Normalization rescales the features so that
they fall within a given range, usually [0, 1]. This ensures that no
particular feature exerts undue influence on the model due to its scale.
In datasets where features have different units or vary widely in
magnitudes, models can become biased, or take longer to train. By
normalizing, we ensure that the optimization algorithms (like gradient
descent) used in these machine learning algorithms converge faster, and
the models become more sensitive to the actual patterns in the data
rather than being influenced by the scales of the features.

```{r echo=TRUE}
library(xgboost)
library(lightgbm)
library(caret)
library(Metrics)

# Sort the data chronologically
wind_turbine_clean_1 <- wind_turbine_clean_1 %>% arrange(Date_time)

# Split the data
train_index <- 1:floor(0.8 * nrow(wind_turbine_clean_1))
train_data <- wind_turbine_clean_1[train_index, ]
test_data <- wind_turbine_clean_1[-train_index, ]

# Define predictor variables and target
predictors <- c("temp", "pressure", "humidity", "wind_speed", "wind_deg", "rain_1h", "snow_1h")
target <- "P_avg"

# Normalize using Min-Max Normalization from the caret package
preProcValues <- preProcess(train_data[, predictors], method = c("range"))
train_data_normalized <- predict(preProcValues, train_data[, predictors])
test_data_normalized <- predict(preProcValues, test_data[, predictors])

# Prepare data for XGBoost
dtrain <- xgb.DMatrix(data = as.matrix(train_data_normalized), label = train_data[[target]])
dtest <- xgb.DMatrix(data = as.matrix(test_data_normalized), label = test_data[[target]])

# Hyperparameter tuning for XGBoost
xgb_params_list <- list(
  list(eta = 0.01, max_depth = 4, objective = "reg:linear"),
  list(eta = 0.1, max_depth = 4, objective = "reg:linear"),
  list(eta = 0.3, max_depth = 4, objective = "reg:linear"),
  list(eta = 0.01, max_depth = 6, objective = "reg:linear"),
  list(eta = 0.1, max_depth = 6, objective = "reg:linear"),
  list(eta = 0.3, max_depth = 6, objective = "reg:linear")
)

best_model <- NULL
best_cv_error <- Inf

for (params in xgb_params_list) {
  cv_model <- xgb.cv(params = params, data = dtrain, nfold = 5, nrounds = 100, early_stopping_rounds = 10, verbose = 0)
  min_error <- min(cv_model$evaluation_log$test_rmse_mean)
  
  if (min_error < best_cv_error) {
    best_cv_error <- min_error
    best_model <- cv_model
    best_param <- params
  }
}

# Train the XGBoost model using the best parameters
xgb_model <- xgb.train(params = best_param, data = dtrain, nrounds = 100)
xgb_predictions <- predict(xgb_model, dtest)

# Prepare data for LightGBM
dtrain_lgb <- lgb.Dataset(data = as.matrix(train_data_normalized), label = train_data[[target]])

# Train the LightGBM model:
lgb_model <- lgb.train(data = dtrain_lgb, objective = "regression", nrounds = 100)
lgb_predictions <- predict(lgb_model, data = as.matrix(test_data_normalized))

# Evaluation
evaluate_model <- function(true_values, predictions) {
  residuals <- true_values - predictions
  sse <- sum(residuals^2)
  sst <- sum((true_values - mean(true_values))^2)
  r2 <- 1 - (sse/sst)

  list(
    RMSE = rmse(true_values, predictions),
    R2 = r2,
    MAE = mae(true_values, predictions),
    Normalized_RMSE = rmse(true_values, predictions) / mean(true_values),
    Normalized_MAE = mae(true_values, predictions) / mean(true_values)
  )
}

xgb_evaluation <- evaluate_model(test_data[[target]], xgb_predictions)
lgb_evaluation <- evaluate_model(test_data[[target]], lgb_predictions)

cat("Evaluation for XGBoost:\n")
print(xgb_evaluation)

cat("\nEvaluation for LightGBM:\n")
print(lgb_evaluation)

# After generating the xgb_predictions
test_data$xgb_predictions <- xgb_predictions

# After generating the lgb_predictions
test_data$lgb_predictions <- lgb_predictions

```

## 7) Metrics:

------------------------------------------------------------------------

**XGBoost Model Evaluation:**

**RMSE (Root Mean Squared Error):** 303

This metric gives an idea of the magnitude of error our model makes in
its predictions. A lower RMSE indicates a better fit of the model to the
data. An RMSE of 303 means that, on average, our predictions deviate
from the actual values by about 303 Kw.

**R\^2 (Coefficient of Determination):** 0.59 (or 59%)

R\^2 measures the proportion of variance in the dependent variable that
is predictable from the independent variables. An R\^2 of 59% suggests
that the model explains approximately 59% of the variability in the
power output.

**MAE (Mean Absolute Error):** 226

MAE represents the average absolute error between the true and predicted
values. An MAE of 226 indicates that, on average, the model's
predictions are off by about 226 Kw.

**Normalized RMSE:** 71%

This is the RMSE scaled by the range of the dependent variable (power
output). It gives a relative idea of the error in terms of the output
range. A 71% normalized error indicates there's room for improvement.

**Normalized MAE:** 53%

This is the mean absolute error scaled by the range of the dependent
variable. A normalized MAE of 53% suggests that our predictions, on
average, are off by about 53% of the output range.

------------------------------------------------------------------------

**LightGBM Model Evaluation:**

**RMSE (Root Mean Squared Error):** 296

The LightGBM model has a slightly lower RMSE than XGBoost, meaning its
predictions are, on average, closer to the actual values by about 296
Kw.

**R\^2 (Coefficient of Determination):** 0.61 (or 61%)

The model explains approximately 61% of the variability in the power
output, which is slightly better than the XGBoost model.

**MAE (Mean Absolute Error):** 222

On average, the model's predictions are off by about 222 Kw, which is
slightly better than the XGBoost model.

**Normalized RMSE:** 69%

The error, in terms of the output range, is slightly better for the
LightGBM model than for XGBoost.

**Normalized MAE:** 52%

Our predictions, on average, are off by about 52% of the output range,
which is slightly better than the XGBoost model.

------------------------------------------------------------------------

**Summary:** Both models demonstrate commendable predictive abilities.
However, the LightGBM model, with an RMSE of 296, has a slight edge over
the XGBoost model, which has an RMSE of 303, across every metric
assessed. Notably, the elevated errors, particularly in the normalized
metrics, suggest room for enhancement and optimization. Exploring
additional features, adjusting model parameters, or considering
alternative modeling strategies might further elevate their performance.

Let's plot the predicted power generated by day versus the power really
generated:

```{r echo=TRUE}
library(tidyr)

# Convert Date_time to Date format, then group by Date and aggregate P_avg
aggregated_data <- test_data %>%
  mutate(Date = as.Date(Date_time)) %>%
  group_by(Date) %>%
  summarise(P_avg_total = sum(P_avg), 
            xgb_predictions_total = sum(xgb_predictions),
            lgb_predictions_total = sum(lgb_predictions))  # Added for LightGBM predictions

# Convert to long format for ggplot2
aggregated_data_long <- aggregated_data %>%
  gather(key = "Type", value = "Value", P_avg_total, xgb_predictions_total, lgb_predictions_total)

# Plot using ggplot2 with every month labeled on the x-axis
ggplot(aggregated_data_long, aes(x = Date, y = Value, color = Type)) + 
  geom_line() +
  labs(title = "Total Power Output (P_avg), XGBoost and LightGBM Predictions per Day",
       x = "Month",
       y = "Value") +
  theme_minimal() +
  scale_x_date(date_breaks = "1 month", date_labels = "%Y-%m") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  scale_color_manual(values = c("blue", "red", "green"))

```

## 8) Conclusion

Through diligent analysis and modeling using advanced machine learning
algorithms, namely XGBoost and LightGBM, we have successfully
demonstrated that weather parameters can be a robust predictor of power
output. Both models yielded satisfactory results, validating our
hypothesis that meteorological conditions play a significant role in
influencing power output.

However, it's pivotal to approach this conclusion with a degree of
caution. Our models were trained and tested using observed weather data,
which inherently is accurate and precise. When transitioning from
observed data to weather forecasts for prediction, the inherent
uncertainties and potential inaccuracies of meteorological predictions
could introduce additional challenges. Predictive weather data, while
continually improving, is not flawless and carries its inherent
prediction errors. Thus, while our current models showcase the viability
of using weather parameters for power output prediction, the performance
might be somewhat attenuated when using weather forecasts instead of
observations.

Nonetheless, this project serves as a strong foundation, highlighting
the potential of leveraging machine learning techniques in the realm of
renewable energy. As weather prediction technology advances and becomes
more accurate, the efficacy of such models will only further improve,
paving the way for more sustainable and predictable energy production.
